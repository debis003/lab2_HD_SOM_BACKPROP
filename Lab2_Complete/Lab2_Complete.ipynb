{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D7041-Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "D7041-Lab 2\n",
    "Deborah Aittoklllio debait-2\n",
    "Joel Willén Joewil-2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Language Classification with High-Dimensional Distributed Representations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.1: Import Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np              # For numerical operations and arrays\n",
    "import os                        # For file and folder operations\n",
    "import re                        # For text cleaning (remove punctuation)\n",
    "from collections import defaultdict  # For storing data in dictionaries\n",
    "import glob                      # For finding files with patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking folder contents...\n",
      "Folder path: C:\\Users\\Deborah Aittokallio\\OneDrive - ltu.se\\University Courses\\Applied Artificial Intelligence D7041E (CURRENT)\\Labs\\Lab 2\\lab2_HD_SOM_BACKPROP\\lab2_HD_SOM_BACKPROP\\News_Languages\n",
      "Folder exists: True\n",
      "Is it a directory: True\n",
      "\n",
      "Total items found: 21\n",
      "\n",
      "First 20 items:\n",
      "  [FOLDER] bul_news_2020_100K\n",
      "  [FOLDER] ces_news_2020_100K\n",
      "  [FOLDER] dan_news_2020_100K\n",
      "  [FOLDER] deu_news_2020_100K\n",
      "  [FOLDER] ell_news_2020_100K\n",
      "  [FOLDER] eng_news_2020_100K\n",
      "  [FOLDER] est_news_2020_100K\n",
      "  [FOLDER] fin_news_2020_100K\n",
      "  [FOLDER] fra_news_2020_100K\n",
      "  [FOLDER] hun_news_2020_100K\n",
      "  [FOLDER] ita_news_2020_100K\n",
      "  [FOLDER] lav_news_2020_100K\n",
      "  [FOLDER] lit_news_2020_100K\n",
      "  [FOLDER] nld_news_2020_100K\n",
      "  [FOLDER] pol_news_2020_100K\n",
      "  [FOLDER] por_news_2020_100K\n",
      "  [FOLDER] ron_news_2020_100K\n",
      "  [FOLDER] slk_news_2020_100K\n",
      "  [FOLDER] slv_news_2020_100K\n",
      "  [FOLDER] spa_news_2020_100K\n"
     ]
    }
   ],
   "source": [
    "# Define the path to my language data folder\n",
    "data_folder = r\"C:\\Users\\Deborah Aittokallio\\OneDrive - ltu.se\\University Courses\\Applied Artificial Intelligence D7041E (CURRENT)\\Labs\\Lab 2\\lab2_HD_SOM_BACKPROP\\lab2_HD_SOM_BACKPROP\\News_Languages\"\n",
    "\n",
    "# Check if the folder exists\n",
    "print(\"Checking folder contents...\")\n",
    "print(f\"Folder path: {data_folder}\")\n",
    "print(f\"Folder exists: {os.path.exists(data_folder)}\")\n",
    "print(f\"Is it a directory: {os.path.isdir(data_folder)}\")\n",
    "\n",
    "# List all files and folders in the directory\n",
    "if os.path.exists(data_folder):\n",
    "    all_items = os.listdir(data_folder)\n",
    "    print(f\"\\nTotal items found: {len(all_items)}\")\n",
    "    print(\"\\nFirst 20 items:\")\n",
    "    for i, item in enumerate(all_items[:20]):\n",
    "        full_path = os.path.join(data_folder, item)\n",
    "        item_type = \"FOLDER\" if os.path.isdir(full_path) else \"FILE\"\n",
    "        print(f\"  [{item_type}] {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load text from a file\n",
    "def load_text_file(file_path):\n",
    "    \"\"\"\n",
    "    Read text from a file\n",
    "    Input: file path (string)\n",
    "    Output: text content (string)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except:\n",
    "        # If UTF-8 doesn't work, try latin-1 encoding\n",
    "        with open(file_path, 'r', encoding='latin-1') as f:\n",
    "            text = f.read()\n",
    "        return \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text (remove punctuation and extra spaces)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove punctuation and convert to lowercase\n",
    "    Input: raw text (string)\n",
    "    Output: cleaned text (string)\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Keep only letters and spaces (remove punctuation, numbers, etc.)\n",
    "    text = re.sub(r'[^a-zäöüßàáâãäåæçèéêëìíîïñòóôõöøùúûüýÿ\\s]', '', text)\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_language_data_from_nested_subfolders(base_folder):\n",
    "    \"\"\"\n",
    "    Load all language text files from nested subfolders\n",
    "    Input: base folder path (string)\n",
    "    Output: dictionary with language names as keys and text as values\n",
    "    \"\"\"\n",
    "    language_data = {}  # Create empty dictionary to store data\n",
    "    \n",
    "    # Get all subfolders in the base folder\n",
    "    subfolders = [f for f in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, f))]\n",
    "    \n",
    "    print(f\"Found {len(subfolders)} language folders\")\n",
    "    \n",
    "    # Loop through each language subfolder\n",
    "    for subfolder in subfolders:\n",
    "        # Extract language code (first 3 letters: bul, eng, fra, etc.)\n",
    "        language_code = subfolder[:3]\n",
    "        \n",
    "        # Path to the language folder\n",
    "        lang_folder_path = os.path.join(base_folder, subfolder)\n",
    "        \n",
    "        # Check if there's a nested folder with the same name\n",
    "        nested_folder_path = os.path.join(lang_folder_path, subfolder)\n",
    "        \n",
    "        # Use nested folder if it exists, otherwise use the main folder\n",
    "        if os.path.isdir(nested_folder_path):\n",
    "            search_path = nested_folder_path\n",
    "        else:\n",
    "            search_path = lang_folder_path\n",
    "        \n",
    "        # Find all .txt files in the search path\n",
    "        txt_files = glob.glob(os.path.join(search_path, \"*.txt\"))\n",
    "        \n",
    "        if len(txt_files) == 0:\n",
    "            print(f\"  ⚠ {language_code}: No .txt files found in {subfolder}\")\n",
    "            continue\n",
    "        \n",
    "        # We'll use the \"sentences.txt\" file if it exists (cleanest data)\n",
    "        # Otherwise combine all .txt files\n",
    "        sentences_file = os.path.join(search_path, f\"{subfolder}-sentences.txt\")\n",
    "        \n",
    "        if os.path.exists(sentences_file):\n",
    "            # Use only the sentences file (best for language classification)\n",
    "            text = load_text_file(sentences_file)\n",
    "            cleaned_text = clean_text(text)\n",
    "            print(f\"  ✓ {language_code}: Loaded sentences file, {len(cleaned_text)} characters\")\n",
    "        else:\n",
    "            # Combine all text files for this language\n",
    "            all_text = \"\"\n",
    "            for txt_file in txt_files:\n",
    "                text = load_text_file(txt_file)\n",
    "                all_text += text + \" \"  # Add space between files\n",
    "            \n",
    "            cleaned_text = clean_text(all_text)\n",
    "            print(f\"  ✓ {language_code}: Loaded {len(txt_files)} file(s), {len(cleaned_text)} characters\")\n",
    "        \n",
    "        # Store in dictionary\n",
    "        language_data[language_code] = cleaned_text\n",
    "    \n",
    "    return language_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading language data from nested subfolders...\n",
      "Found 21 language folders\n",
      "  ✓ bul: Loaded sentences file, 39908 characters\n",
      "  ✓ ces: Loaded sentences file, 8541308 characters\n",
      "  ✓ dan: Loaded sentences file, 10426448 characters\n",
      "  ✓ deu: Loaded sentences file, 10360606 characters\n",
      "  ✓ ell: Loaded sentences file, 180880 characters\n",
      "  ✓ eng: Loaded sentences file, 11234400 characters\n",
      "  ✓ est: Loaded sentences file, 10606005 characters\n",
      "  ✓ fin: Loaded sentences file, 9620216 characters\n",
      "  ✓ fra: Loaded sentences file, 11535536 characters\n",
      "  ✓ hun: Loaded sentences file, 11758448 characters\n",
      "  ✓ ita: Loaded sentences file, 11654444 characters\n",
      "  ✓ lav: Loaded sentences file, 10856836 characters\n",
      "  ✓ lit: Loaded sentences file, 9894916 characters\n",
      "  ✓ nld: Loaded sentences file, 8593819 characters\n",
      "  ✓ pol: Loaded sentences file, 8804624 characters\n",
      "  ✓ por: Loaded sentences file, 11499232 characters\n",
      "  ✓ ron: Loaded sentences file, 10705686 characters\n",
      "  ✓ slk: Loaded sentences file, 9667070 characters\n",
      "  ✓ slv: Loaded sentences file, 10836893 characters\n",
      "  ✓ spa: Loaded sentences file, 13314097 characters\n",
      "  ✓ swe: Loaded sentences file, 9468757 characters\n",
      "\n",
      "✓ Total languages loaded: 21\n",
      "Languages: ['bul', 'ces', 'dan', 'deu', 'ell', 'eng', 'est', 'fin', 'fra', 'hun', 'ita', 'lav', 'lit', 'nld', 'pol', 'por', 'ron', 'slk', 'slv', 'spa', 'swe']\n",
      "\n",
      "Splitting data into train/validation/test sets (70%/15%/15%)...\n",
      "  bul: Train=27935 | Val=5986 | Test=5987 characters\n",
      "  ces: Train=5978915 | Val=1281196 | Test=1281197 characters\n",
      "  dan: Train=7298513 | Val=1563967 | Test=1563968 characters\n",
      "  deu: Train=7252424 | Val=1554090 | Test=1554092 characters\n",
      "  ell: Train=126615 | Val=27132 | Test=27133 characters\n",
      "  eng: Train=7864079 | Val=1685160 | Test=1685161 characters\n",
      "  est: Train=7424203 | Val=1590900 | Test=1590902 characters\n",
      "  fin: Train=6734151 | Val=1443032 | Test=1443033 characters\n",
      "  fra: Train=8074875 | Val=1730330 | Test=1730331 characters\n",
      "  hun: Train=8230913 | Val=1763767 | Test=1763768 characters\n",
      "  ita: Train=8158110 | Val=1748166 | Test=1748168 characters\n",
      "  lav: Train=7599785 | Val=1628525 | Test=1628526 characters\n",
      "  lit: Train=6926441 | Val=1484237 | Test=1484238 characters\n",
      "  nld: Train=6015673 | Val=1289072 | Test=1289074 characters\n",
      "  pol: Train=6163236 | Val=1320693 | Test=1320695 characters\n",
      "  por: Train=8049462 | Val=1724884 | Test=1724886 characters\n",
      "  ron: Train=7493980 | Val=1605852 | Test=1605854 characters\n",
      "  slk: Train=6766949 | Val=1450060 | Test=1450061 characters\n",
      "  slv: Train=7585825 | Val=1625533 | Test=1625535 characters\n",
      "  spa: Train=9319867 | Val=1997114 | Test=1997116 characters\n",
      "  swe: Train=6628129 | Val=1420313 | Test=1420315 characters\n",
      "\n",
      "✓ Data loading and splitting complete!\n"
     ]
    }
   ],
   "source": [
    "# Load all language data from nested subfolders\n",
    "print(\"Loading language data from nested subfolders...\")\n",
    "language_texts = load_language_data_from_nested_subfolders(data_folder)\n",
    "print(f\"\\n✓ Total languages loaded: {len(language_texts)}\")\n",
    "print(f\"Languages: {sorted(list(language_texts.keys()))}\")\n",
    "\n",
    "# Split each language's data into train/val/test\n",
    "train_data = {}  # Dictionary to store training data for each language\n",
    "val_data = {}    # Dictionary to store validation data for each language\n",
    "test_data = {}   # Dictionary to store test data for each language\n",
    "\n",
    "print(\"\\nSplitting data into train/validation/test sets (70%/15%/15%)...\")\n",
    "\n",
    "for language, text in language_texts.items():\n",
    "    # Split the text for this language\n",
    "    train_text, val_text, test_text = split_data(text)\n",
    "    \n",
    "    # Store the splits\n",
    "    train_data[language] = train_text\n",
    "    val_data[language] = val_text\n",
    "    test_data[language] = test_text\n",
    "    \n",
    "    print(f\"  {language}: Train={len(train_text)} | Val={len(val_text)} | Test={len(test_text)} characters\")\n",
    "\n",
    "print(\"\\n✓ Data loading and splitting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample from 'bul' training data (first 200 characters):\n",
      "kritikar aaaa abbvie astrazeneca aci iata adobe magento leader magic quadrant gartner it airbnb ako ako allianz amazon anna koleva kadirova anthrax spreading the disease accept pandemic anton petrov a\n"
     ]
    }
   ],
   "source": [
    "# Display a sample from one language to verify\n",
    "sample_language = sorted(list(train_data.keys()))[0]  # Get first language alphabetically\n",
    "print(f\"\\nSample from '{sample_language}' training data (first 200 characters):\")\n",
    "print(train_data[sample_language][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "\n",
    "\"\"\"Import libraries (numpy, os, re, etc.) ✓\n",
    "Load News Wortschatz Corpora ✓\n",
    "Load Euro Parliament Parallel Corpus\n",
    "Preprocess data (remove punctuation, etc.) ✓\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2: Constructing High-Dimensional Centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "\"\"\"\n",
    "Implement n-gram encoding (n=3, tri-grams) \n",
    "Create HD vectors with d=100 and d=1000\n",
    "Build language centroids (21 languages)\n",
    "Answer questions about conventional n-gram representations\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.3: Classification using Hyperdimensional Centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "\"\"\"\n",
    "Implement cosine similarity\n",
    "Classify text samples\n",
    "Display confusion matrix\n",
    "Calculate accuracy and F1-score\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Unsupervised Learning with Self-Organizing \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.1: Unsupervised Learning of Hand-Written Digits with SOM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy SOM code from D7041E-lab4_SOM.ipynb\n",
    "# TODO: Adapt code to use MNIST instead of zoo.txt\n",
    "\n",
    "\"\"\"\n",
    "# Step 1: Load the MNIST dataset\n",
    "# TODO: Load MNIST\n",
    "\n",
    "# Step 2: Use the flattened (1D) array of pixels of each image as a feature vector\n",
    "# TODO: Flatten images to 784 features\n",
    "\n",
    "# Step 3: Initialize weights in SOM network randomly, train SOM with grid sizes\n",
    "# TODO: Train SOM with grid 20x20\n",
    "# TODO: Train SOM with grid 40x40\n",
    "# TODO: Train SOM with grid 80x80\n",
    "\n",
    "# Step 4: Display initial, intermediate (at 50%), and final learned weights\n",
    "# TODO: Display initial weights as 28x28 images\n",
    "# TODO: Display intermediate weights (at 50% of iterations) as 28x28 images\n",
    "# TODO: Display final learned weights as 28x28 images\n",
    "\n",
    "# Step 5: Assign labels to neurons by passing TRAINING examples through trained SOM\n",
    "# TODO: Pass training examples and record statistics\n",
    "# TODO: Assign labels to neurons\n",
    "# TODO: Display confusion matrix for TRAINING SET\n",
    "# TODO: Display confusion matrix for TEST SET\n",
    "\n",
    "# Step 6: Experiment with learning rate\n",
    "# TODO: Increase learning rate (fixed iterations)\n",
    "# TODO: Decrease learning rate (fixed iterations)\n",
    "# TODO: Answer: What is the resulting effect?\n",
    "\n",
    "\n",
    "## Question 6: What is the resulting effect of changing learning rate? Answer: [YOUR ANSWER HERE]\n",
    "\n",
    "# Step 7: Experiment with neighborhood decay\n",
    "# TODO: For fixed iterations and best learning rate, increase exponential decay of neighbourhood parameter\n",
    "# TODO: For fixed iterations and best learning rate, decrease exponential decay of neighbourhood parameter\n",
    "\n",
    "\n",
    "## Question 8: What is the effect? Answer: [YOUR ANSWER HERE] \n",
    "## Question 9: What is a biological neuron? How does it relate to the concept of neurons in SOM? Answer: [YOUR ANSWER HERE]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Fundamentals of Artificial Neural Networks and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.1: Multi-layer perceptron and backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy backpropagation code from ANN_backprop.ipynb\n",
    "\n",
    "\n",
    "### Task 3.1.1: Understand the implementation structure of the multilayer \n",
    "\"\"\"\n",
    "Task 3.1.a: Be able to explain the principle of backpropagation algorithm Answer: [YOUR ANSWER HERE] \n",
    "Task 3.1.b: Be able to explain the meaning and the role of the Softmax function Answer: [YOUR ANSWER HERE] \n",
    "Task 3.1.c: Be able to name typically used non-linear output functions and implications of choosing one or another for implementation Answer: [YOUR ANSWER HERE] \n",
    "Task 3.1.d: Find the places in the code where execution breaks, answer the questions, comment out the exit line\n",
    "# TODO: Find Question 1 in backpropagate() method\n",
    "# TODO: Answer Question 1: What is computed in the next line of code?\n",
    "\n",
    "\n",
    "Question 1: What is computed in the delta calculation? Answer: [YOUR ANSWER HERE]\n",
    "# TODO: Comment out exit_with_err() for Question 1\n",
    "# TODO: Find Question 2 in backpropagate() method\n",
    "# TODO: Answer Question 2: What does this 'for' loop do?\n",
    "\n",
    "Question 2: What does the backpropagation 'for' loop do? Answer: [YOUR ANSWER HERE]\n",
    "# TODO: Comment out exit_with_err() for Question 2\n",
    "# TODO: Find Question 3 in evaluate() method\n",
    "# TODO: Answer Question 3: How is weight update implemented? What is eta?\n",
    "\n",
    "Question 3: How is weight update implemented? What is eta? Answer: [YOUR ANSWER HERE]\n",
    "# TODO: Comment out exit_with_err() for Question 3\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3.1.2: Run with default hyperparameters\n",
    "# TODO: Run with epochs=70, learning_rate=0.05\n",
    "# TODO: Record classification \n",
    "\n",
    "\"\"\"\n",
    "Question: What is the classification accuracy? Answer: [YOUR ANSWER HERE]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3.1.3: Run with different learning rates\n",
    "# TODO: Run with learning_rate=0.005\n",
    "# TODO: Run with learning_rate=0.5\n",
    "# TODO: Compare results\n",
    "\"\"\"\n",
    "Question: Explain the observed differences in the functionality of the multi-layer perceptron Answer: [YOUR ANSWER HERE] \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3.1.4: Implement ReLU activation function\n",
    "# TODO: Implement f_relu() function with forward pass\n",
    "# TODO: Implement f_relu() derivative\n",
    "# TODO: Run perceptron with ReLU, epochs=70, learning_rate=0.05\n",
    "# TODO: Record classification accuracy with ReLU\n",
    "# TODO: Find learning rate values that give comparable accuracy to Sigmoid\n",
    "\n",
    "\"\"\"\n",
    "Question: What is the classification accuracy with ReLU? Answer: [YOUR ANSWER HERE] Question: What learning rate gives comparable accuracy to Sigmoid? Answer: [YOUR ANSWER HERE]\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
