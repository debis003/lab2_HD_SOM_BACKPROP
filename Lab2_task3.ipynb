{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p6jZWO2e8ouM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2xmNDsv08ouO"
      },
      "outputs": [],
      "source": [
        "#functions of non-linear activations\n",
        "def f_sigmoid(X, deriv=False):\n",
        "    if not deriv:\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "    else:\n",
        "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
        "\n",
        "\n",
        "def f_softmax(X):\n",
        "    Z = np.sum(np.exp(X), axis=1)\n",
        "    Z = Z.reshape(Z.shape[0], 1)\n",
        "    return np.exp(X) / Z\n",
        "\n",
        "def f_relu(X, deriv=False):\n",
        "    if not deriv:\n",
        "        return np.maximum(0, X)\n",
        "    else:\n",
        "        # derivative: 1 if X > 0, else 0\n",
        "        return (X > 0).astype(float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oXL7RisI8ouO"
      },
      "outputs": [],
      "source": [
        "def exit_with_err(err_str):\n",
        "    print(sys.stderr, err_str)\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wj3vf93_8ouP"
      },
      "outputs": [],
      "source": [
        "#Functionality of a single hidden layer\n",
        "class Layer:\n",
        "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
        "                 activation=f_sigmoid):\n",
        "        self.is_input = is_input\n",
        "        self.is_output = is_output\n",
        "\n",
        "        # Z is the matrix that holds output values\n",
        "        self.Z = np.zeros((batch_size, size[0]))\n",
        "        # The activation function is an externally defined function (with a\n",
        "        # derivative) that is stored here\n",
        "        self.activation = activation\n",
        "\n",
        "        # W is the outgoing weight matrix for this layer\n",
        "        self.W = None\n",
        "        # S is the matrix that holds the inputs to this layer\n",
        "        self.S = None\n",
        "        # D is the matrix that holds the deltas for this layer\n",
        "        self.D = None\n",
        "        # Fp is the matrix that holds the derivatives of the activation function\n",
        "        self.Fp = None\n",
        "\n",
        "        if not is_input:\n",
        "            self.S = np.zeros((batch_size, size[0]))\n",
        "            self.D = np.zeros((batch_size, size[0]))\n",
        "\n",
        "        if not is_output:\n",
        "            self.W = np.random.normal(size=size, scale=1E-4)\n",
        "\n",
        "        if not is_input and not is_output:\n",
        "            self.Fp = np.zeros((size[0], batch_size))\n",
        "\n",
        "    def forward_propagate(self):\n",
        "        if self.is_input:\n",
        "            return self.Z.dot(self.W)\n",
        "\n",
        "        self.Z = self.activation(self.S)\n",
        "        if self.is_output:\n",
        "            return self.Z\n",
        "        else:\n",
        "            # For hidden layers, we add the bias values here\n",
        "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
        "            self.Fp = self.activation(self.S, deriv=True).T\n",
        "            return self.Z.dot(self.W)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "76C-Pasm8ouP"
      },
      "outputs": [],
      "source": [
        "class MultiLayerPerceptron:\n",
        "\n",
        "    def __init__(self, layer_config, batch_size=100, hidden_activation=f_sigmoid):\n",
        "        self.layers = []\n",
        "        self.num_layers = len(layer_config)\n",
        "        self.minibatch_size = batch_size\n",
        "\n",
        "        for i in range(self.num_layers-1):\n",
        "            if i == 0:\n",
        "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
        "                # Input layer (we add one unit for bias in Z, but no activation here)\n",
        "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
        "                                         batch_size,\n",
        "                                         is_input=True))\n",
        "            else:\n",
        "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
        "                # Hidden layers use the chosen activation (sigmoid or ReLU)\n",
        "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
        "                                         batch_size,\n",
        "                                         activation=hidden_activation))\n",
        "\n",
        "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
        "        # Output layer still uses softmax\n",
        "        self.layers.append(Layer([layer_config[-1], None],\n",
        "                                 batch_size,\n",
        "                                 is_output=True,\n",
        "                                 activation=f_softmax))\n",
        "        print (\"Done!\")\n",
        "\n",
        "    def forward_propagate(self, data):\n",
        "        #We need to be sure to add bias values to the input\n",
        "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
        "\n",
        "        for i in range(self.num_layers-1):\n",
        "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
        "        return self.layers[-1].forward_propagate()\n",
        "\n",
        "    def backpropagate(self, yhat, labels):\n",
        "\n",
        "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
        "        #This is the where delta (yhat-labels) is calculated. The start of backpropagation\n",
        "\n",
        "        self.layers[-1].D = (yhat - labels).T\n",
        "        for i in range(self.num_layers-2, 0, -1):\n",
        "            # We do not calculate deltas for the bias values\n",
        "            W_nobias = self.layers[i].W[0:-1, :]\n",
        "\n",
        "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
        "            #This loop iterates through each hidden layer and implements the backpropogation step for the layer\n",
        "\n",
        "\n",
        "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp #This being the backpropogation formula\n",
        "\n",
        "    def update_weights(self, eta):\n",
        "        for i in range(0, self.num_layers-1):\n",
        "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
        "            self.layers[i].W += W_grad\n",
        "\n",
        "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
        "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
        "\n",
        "        N_train = len(train_labels)*len(train_labels[0])\n",
        "        N_test = len(test_labels)*len(test_labels[0])\n",
        "\n",
        "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
        "        for t in range(0, num_epochs):\n",
        "            out_str = \"[{0:4d}] \".format(t)\n",
        "\n",
        "            for b_data, b_labels in zip(train_data, train_labels):\n",
        "                output = self.forward_propagate(b_data)\n",
        "                self.backpropagate(output, b_labels)\n",
        "\n",
        "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
        "                #eta is the learning rate and the weight update is implemented with the update weights function which contains the formula for gradient descent\n",
        "\n",
        "                self.update_weights(eta=eta)\n",
        "\n",
        "            if eval_train:\n",
        "                errs = 0\n",
        "                for b_data, b_labels in zip(train_data, train_labels):\n",
        "                    output = self.forward_propagate(b_data)\n",
        "                    yhat = np.argmax(output, axis=1)\n",
        "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
        "\n",
        "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
        "                                                           float(errs)/N_train))\n",
        "\n",
        "            if eval_test:\n",
        "                errs = 0\n",
        "                for b_data, b_labels in zip(test_data, test_labels):\n",
        "                    output = self.forward_propagate(b_data)\n",
        "                    yhat = np.argmax(output, axis=1)\n",
        "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
        "\n",
        "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
        "                                                       float(errs)/N_test)\n",
        "\n",
        "            print (out_str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jl2FsN-48ouP"
      },
      "outputs": [],
      "source": [
        "def label_to_bit_vector(labels, nbits):\n",
        "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
        "    for i in range(labels.shape[0]):\n",
        "        bit_vector[i, labels[i]] = 1.0\n",
        "\n",
        "    return bit_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "g8TcebH58ouQ"
      },
      "outputs": [],
      "source": [
        "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
        "    N = data.shape[0]\n",
        "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
        "\n",
        "    if N % batch_size != 0:\n",
        "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
        "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
        "    chunked_data = []\n",
        "    chunked_labels = []\n",
        "    idx = 0\n",
        "    while idx + batch_size <= N:\n",
        "        chunked_data.append(data[idx:idx+batch_size, :])\n",
        "        if not create_bit_vector:\n",
        "            chunked_labels.append(labels[idx:idx+batch_size])\n",
        "        else:\n",
        "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
        "            chunked_labels.append(bit_vector)\n",
        "\n",
        "        idx += batch_size\n",
        "\n",
        "    return chunked_data, chunked_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aH2mQ_5j8ouQ"
      },
      "outputs": [],
      "source": [
        "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
        "\n",
        "    print (\"Creating data...\")\n",
        "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
        "                                              batch_size,\n",
        "                                              create_bit_vector=True)\n",
        "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
        "                                              batch_size,\n",
        "                                              create_bit_vector=True)\n",
        "    print (\"Done!\")\n",
        "\n",
        "\n",
        "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWTYgg978ouQ"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paNsmAzS8ouR",
        "outputId": "7d2fba62-5d51-4de7-d5da-e9625bfb311d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
        "\n",
        "Xtr = Xtr.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "Xtr = Xtr.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "Xtr /= 255\n",
        "X_test /= 255\n",
        "print(Xtr.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.a) The principle of the backpropogation algorithm; The backpropogation can be explained by three components that make use of the chain rule. The first being the forward propogation which stores the activations, pre activations and the derivative of activations. The Backpropogation component which is the calculation of the delta at the output layer and then iterates backwards over each hidden layers and computes their deltas using that output delta and then lastly the weight update which updates the weights using gradiant descent.\n",
        "\n",
        "1.b) Softmax meaning is to take a vector of scores or logits and turn these to probability distribution over classes. Which means all outputs sum to 1. As such its well suited for multiclass classification tasks. In the given MLP code its role is to take logits and turn them into probabilities over classes. ANd even more so it is part of the cross entropy loss used.\n",
        "\n",
        "1.c) In this code softmax is used on the output to get multi class classification and sigmoid activation functions in the hidden layers which add nonlinearity.\n",
        "Other examples are a linear output or Relu and Mean square error for regression. Which ones you choose to use in the code is very tied to the problem itself and choosing the correct activation functions for the task at hand is vital for the network to be able to suceed.\n",
        "\n",
        "1.d) Comments have been added next to the raised errors to answer there questions. See the MLP class."
      ],
      "metadata": {
        "id": "YZ8njssjCSe4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6jfjq3O8ouR",
        "outputId": "657607a6-7650-41d5-8a75-5e651c7d9d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data...\n",
            "Batch size 100, the number of examples 60000.\n",
            "Batch size 100, the number of examples 10000.\n",
            "Done!\n",
            "Initializing input layer with size 784.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing output layer with size 10.\n",
            "Done!\n",
            "Training for 70 epochs...\n",
            "[   0]  Training error: 0.45165 Test error: 0.45930\n",
            "[   1]  Training error: 0.07577 Test error: 0.07490\n",
            "[   2]  Training error: 0.07840 Test error: 0.07970\n",
            "[   3]  Training error: 0.04012 Test error: 0.04340\n",
            "[   4]  Training error: 0.03302 Test error: 0.03950\n",
            "[   5]  Training error: 0.03097 Test error: 0.03980\n",
            "[   6]  Training error: 0.02888 Test error: 0.03890\n",
            "[   7]  Training error: 0.02595 Test error: 0.03770\n",
            "[   8]  Training error: 0.02448 Test error: 0.03580\n",
            "[   9]  Training error: 0.02137 Test error: 0.03400\n",
            "[  10]  Training error: 0.02015 Test error: 0.03330\n",
            "[  11]  Training error: 0.01828 Test error: 0.03340\n",
            "[  12]  Training error: 0.01862 Test error: 0.03270\n",
            "[  13]  Training error: 0.01652 Test error: 0.03160\n",
            "[  14]  Training error: 0.01535 Test error: 0.03230\n",
            "[  15]  Training error: 0.01865 Test error: 0.03260\n",
            "[  16]  Training error: 0.01767 Test error: 0.03460\n",
            "[  17]  Training error: 0.01153 Test error: 0.03340\n",
            "[  18]  Training error: 0.01383 Test error: 0.03290\n",
            "[  19]  Training error: 0.01013 Test error: 0.03010\n",
            "[  20]  Training error: 0.00890 Test error: 0.03030\n",
            "[  21]  Training error: 0.01597 Test error: 0.03430\n",
            "[  22]  Training error: 0.01552 Test error: 0.03510\n",
            "[  23]  Training error: 0.01143 Test error: 0.03360\n",
            "[  24]  Training error: 0.00888 Test error: 0.03120\n",
            "[  25]  Training error: 0.00690 Test error: 0.02890\n",
            "[  26]  Training error: 0.00632 Test error: 0.02870\n",
            "[  27]  Training error: 0.00717 Test error: 0.02960\n",
            "[  28]  Training error: 0.00660 Test error: 0.02890\n",
            "[  29]  Training error: 0.00648 Test error: 0.02850\n",
            "[  30]  Training error: 0.01148 Test error: 0.03280\n",
            "[  31]  Training error: 0.01110 Test error: 0.03260\n",
            "[  32]  Training error: 0.00697 Test error: 0.02990\n",
            "[  33]  Training error: 0.00810 Test error: 0.03110\n",
            "[  34]  Training error: 0.00825 Test error: 0.03230\n",
            "[  35]  Training error: 0.00680 Test error: 0.03030\n",
            "[  36]  Training error: 0.00572 Test error: 0.02980\n",
            "[  37]  Training error: 0.00540 Test error: 0.02850\n",
            "[  38]  Training error: 0.00357 Test error: 0.02840\n",
            "[  39]  Training error: 0.00253 Test error: 0.02800\n",
            "[  40]  Training error: 0.00227 Test error: 0.02750\n",
            "[  41]  Training error: 0.00323 Test error: 0.02730\n",
            "[  42]  Training error: 0.00165 Test error: 0.02660\n",
            "[  43]  Training error: 0.00185 Test error: 0.02700\n",
            "[  44]  Training error: 0.00870 Test error: 0.03140\n",
            "[  45]  Training error: 0.00673 Test error: 0.03000\n",
            "[  46]  Training error: 0.00790 Test error: 0.03030\n",
            "[  47]  Training error: 0.00495 Test error: 0.02940\n",
            "[  48]  Training error: 0.01010 Test error: 0.03170\n",
            "[  49]  Training error: 0.00328 Test error: 0.02940\n",
            "[  50]  Training error: 0.00272 Test error: 0.02800\n",
            "[  51]  Training error: 0.00220 Test error: 0.02850\n",
            "[  52]  Training error: 0.00207 Test error: 0.02880\n",
            "[  53]  Training error: 0.00385 Test error: 0.03000\n",
            "[  54]  Training error: 0.00143 Test error: 0.02830\n",
            "[  55]  Training error: 0.00122 Test error: 0.02710\n",
            "[  56]  Training error: 0.00267 Test error: 0.02840\n",
            "[  57]  Training error: 0.00063 Test error: 0.02630\n",
            "[  58]  Training error: 0.00123 Test error: 0.02760\n",
            "[  59]  Training error: 0.00247 Test error: 0.02900\n",
            "[  60]  Training error: 0.00208 Test error: 0.02800\n",
            "[  61]  Training error: 0.00060 Test error: 0.02610\n",
            "[  62]  Training error: 0.00087 Test error: 0.02800\n",
            "[  63]  Training error: 0.00052 Test error: 0.02660\n",
            "[  64]  Training error: 0.00010 Test error: 0.02670\n",
            "[  65]  Training error: 0.00002 Test error: 0.02630\n",
            "[  66]  Training error: 0.00002 Test error: 0.02650\n",
            "[  67]  Training error: 0.00000 Test error: 0.02650\n",
            "[  68]  Training error: 0.00000 Test error: 0.02670\n",
            "[  69]  Training error: 0.00000 Test error: 0.02630\n",
            "\n",
            "FINAL ACCURACIES:\n",
            "Training accuracy:   100.00%\n",
            "Validation accuracy: 97.37%\n",
            "Done:)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size=100;\n",
        "\n",
        "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
        "\n",
        "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
        "\n",
        "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
        "             eval_train=True)\n",
        "\n",
        "#Training accuracy\n",
        "train_correct = 0\n",
        "train_total = 0\n",
        "for b_data, b_labels in zip(train_data, train_labels):\n",
        "    output = mlp.forward_propagate(b_data)\n",
        "    yhat = np.argmax(output, axis=1)\n",
        "    ytrue = np.argmax(b_labels, axis=1)\n",
        "    train_correct += np.sum(yhat == ytrue)\n",
        "    train_total += len(ytrue)\n",
        "train_accuracy = train_correct / train_total\n",
        "\n",
        "#Validation accuracy\n",
        "valid_correct = 0\n",
        "valid_total = 0\n",
        "for b_data, b_labels in zip(valid_data, valid_labels):\n",
        "    output = mlp.forward_propagate(b_data)\n",
        "    yhat = np.argmax(output, axis=1)\n",
        "    ytrue = np.argmax(b_labels, axis=1)\n",
        "    valid_correct += np.sum(yhat == ytrue)\n",
        "    valid_total += len(ytrue)\n",
        "valid_accuracy = valid_correct / valid_total\n",
        "\n",
        "print(\"\\nFINAL ACCURACIES:\")\n",
        "print(f\"Training accuracy:   {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {valid_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "print(\"Done:)\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2. We get a training accuracy of 100% and validation accuracy of 97.32%. This took 3 minutes.\n",
        "\n",
        "Part 3. For an lr of 0.005 we get a Training accuracy of 99.97% and Validation of 97.31%. This took 6 minutes. Lastly for an lr of 0.5 we get a Training accuracy of 9.75% and a validation accuracy of 9.74%. This took 6 minutes.\n",
        "\n",
        "What we can extrapolate from these results is that an lr of 0.005 is able to get the same results as lr 0.05 but with double the time. This is because the weight updates are considerably smaller making convergence slower but with time is able to get almost the same accuracy as our best case.\n",
        "In the case of lr of 0.5 we can see that the MLP \"collpases\" as the optimal values for weights are overshot at each step and our gradents vanish. This leaves the accuracy up to essentially random guess. Giving us the very bad accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "MuP9EbWMSDgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4. With the same parameters as the best case previously, we get the same problem the lr of 0.5 got earlier. Around 10%. As such it is clear we must lower the lr to get any meaningful result. As such we lowered it by a considerable margin to 0.005. Which turned out to be the lucky number as it gave us the same accuracy value as that of the best case of Sigmoid,"
      ],
      "metadata": {
        "id": "9JoUWBggGZtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "train_data, train_labels, valid_data, valid_labels = prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
        "\n",
        "#ReLU MLP\n",
        "mlp_relu = MultiLayerPerceptron(layer_config=[784, 100, 100, 10],batch_size=batch_size,hidden_activation=f_relu)\n",
        "\n",
        "mlp_relu.evaluate(train_data, train_labels, valid_data, valid_labels,num_epochs=70, eta=0.005, eval_train=True)\n",
        "\n",
        "#Training accuracy\n",
        "train_correct = 0\n",
        "train_total = 0\n",
        "for b_data, b_labels in zip(train_data, train_labels):\n",
        "    output = mlp_relu.forward_propagate(b_data)\n",
        "    yhat = np.argmax(output, axis=1)\n",
        "    ytrue = np.argmax(b_labels, axis=1)\n",
        "    train_correct += np.sum(yhat == ytrue)\n",
        "    train_total += len(ytrue)\n",
        "train_accuracy = train_correct / train_total\n",
        "\n",
        "#Validation accuracy\n",
        "valid_correct = 0\n",
        "valid_total = 0\n",
        "for b_data, b_labels in zip(valid_data, valid_labels):\n",
        "    output = mlp_relu.forward_propagate(b_data)\n",
        "    yhat = np.argmax(output, axis=1)\n",
        "    ytrue = np.argmax(b_labels, axis=1)\n",
        "    valid_correct += np.sum(yhat == ytrue)\n",
        "    valid_total += len(ytrue)\n",
        "valid_accuracy = valid_correct / valid_total\n",
        "\n",
        "print(\"\\nReLU FINAL ACCURACIES:\")\n",
        "print(f\"Training accuracy:   {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {valid_accuracy*100:.2f}%\")\n",
        "\n",
        "print(\"Done:)\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwUjchNpGaDV",
        "outputId": "c561df81-400a-48e6-ec42-8eb4cff26bcc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data...\n",
            "Batch size 100, the number of examples 60000.\n",
            "Batch size 100, the number of examples 10000.\n",
            "Done!\n",
            "Initializing input layer with size 784.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing output layer with size 10.\n",
            "Done!\n",
            "Training for 70 epochs...\n",
            "[   0]  Training error: 0.89558 Test error: 0.89720\n",
            "[   1]  Training error: 0.89558 Test error: 0.89720\n",
            "[   2]  Training error: 0.89558 Test error: 0.89720\n",
            "[   3]  Training error: 0.89558 Test error: 0.89720\n",
            "[   4]  Training error: 0.89558 Test error: 0.89720\n",
            "[   5]  Training error: 0.89558 Test error: 0.89720\n",
            "[   6]  Training error: 0.89558 Test error: 0.89720\n",
            "[   7]  Training error: 0.89558 Test error: 0.89720\n",
            "[   8]  Training error: 0.89558 Test error: 0.89720\n",
            "[   9]  Training error: 0.89558 Test error: 0.89720\n",
            "[  10]  Training error: 0.89558 Test error: 0.89720\n",
            "[  11]  Training error: 0.89558 Test error: 0.89720\n",
            "[  12]  Training error: 0.89558 Test error: 0.89720\n",
            "[  13]  Training error: 0.89558 Test error: 0.89720\n",
            "[  14]  Training error: 0.89558 Test error: 0.89720\n",
            "[  15]  Training error: 0.89558 Test error: 0.89720\n",
            "[  16]  Training error: 0.78302 Test error: 0.78710\n",
            "[  17]  Training error: 0.07975 Test error: 0.07950\n",
            "[  18]  Training error: 0.04952 Test error: 0.05190\n",
            "[  19]  Training error: 0.04667 Test error: 0.05300\n",
            "[  20]  Training error: 0.03177 Test error: 0.03640\n",
            "[  21]  Training error: 0.02875 Test error: 0.03700\n",
            "[  22]  Training error: 0.02545 Test error: 0.03670\n",
            "[  23]  Training error: 0.02288 Test error: 0.03480\n",
            "[  24]  Training error: 0.02258 Test error: 0.03590\n",
            "[  25]  Training error: 0.01877 Test error: 0.03400\n",
            "[  26]  Training error: 0.01767 Test error: 0.03400\n",
            "[  27]  Training error: 0.01547 Test error: 0.03330\n",
            "[  28]  Training error: 0.01965 Test error: 0.03660\n",
            "[  29]  Training error: 0.01462 Test error: 0.03180\n",
            "[  30]  Training error: 0.01472 Test error: 0.03320\n",
            "[  31]  Training error: 0.01065 Test error: 0.02900\n",
            "[  32]  Training error: 0.01708 Test error: 0.03490\n",
            "[  33]  Training error: 0.01170 Test error: 0.03020\n",
            "[  34]  Training error: 0.01397 Test error: 0.03230\n",
            "[  35]  Training error: 0.01000 Test error: 0.02940\n",
            "[  36]  Training error: 0.01208 Test error: 0.03240\n",
            "[  37]  Training error: 0.00727 Test error: 0.02860\n",
            "[  38]  Training error: 0.00577 Test error: 0.02680\n",
            "[  39]  Training error: 0.01133 Test error: 0.03050\n",
            "[  40]  Training error: 0.00383 Test error: 0.02480\n",
            "[  41]  Training error: 0.00950 Test error: 0.03020\n",
            "[  42]  Training error: 0.00480 Test error: 0.02780\n",
            "[  43]  Training error: 0.00968 Test error: 0.02960\n",
            "[  44]  Training error: 0.00520 Test error: 0.02650\n",
            "[  45]  Training error: 0.00583 Test error: 0.02810\n",
            "[  46]  Training error: 0.00848 Test error: 0.02950\n",
            "[  47]  Training error: 0.00458 Test error: 0.02740\n",
            "[  48]  Training error: 0.00402 Test error: 0.02540\n",
            "[  49]  Training error: 0.00503 Test error: 0.02880\n",
            "[  50]  Training error: 0.00773 Test error: 0.03140\n",
            "[  51]  Training error: 0.00172 Test error: 0.02390\n",
            "[  52]  Training error: 0.00350 Test error: 0.02550\n",
            "[  53]  Training error: 0.00245 Test error: 0.02520\n",
            "[  54]  Training error: 0.00143 Test error: 0.02460\n",
            "[  55]  Training error: 0.00068 Test error: 0.02380\n",
            "[  56]  Training error: 0.00048 Test error: 0.02370\n",
            "[  57]  Training error: 0.00020 Test error: 0.02320\n",
            "[  58]  Training error: 0.00015 Test error: 0.02250\n",
            "[  59]  Training error: 0.00008 Test error: 0.02240\n",
            "[  60]  Training error: 0.00000 Test error: 0.02220\n",
            "[  61]  Training error: 0.00005 Test error: 0.02220\n",
            "[  62]  Training error: 0.00003 Test error: 0.02200\n",
            "[  63]  Training error: 0.00000 Test error: 0.02180\n",
            "[  64]  Training error: 0.00000 Test error: 0.02160\n",
            "[  65]  Training error: 0.00000 Test error: 0.02160\n",
            "[  66]  Training error: 0.00000 Test error: 0.02150\n",
            "[  67]  Training error: 0.00000 Test error: 0.02140\n",
            "[  68]  Training error: 0.00000 Test error: 0.02160\n",
            "[  69]  Training error: 0.00000 Test error: 0.02150\n",
            "\n",
            "ReLU FINAL ACCURACIES:\n",
            "Training accuracy:   100.00%\n",
            "Validation accuracy: 97.85%\n",
            "Done:)\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}